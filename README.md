# JMLE-LLM-Toolkit

## Overview

This repository provides a toolkit to support the development of Large Language Models (LLMs) specialized for the Japanese medical domain, with a particular focus on data from the **Japanese Medical Licensing Examination (JMLE)** past questions. It offers scripts and notebooks for generating and processing LLM training datasets (Chain-of-Thought, Direct Preference Optimization) from JMLE data.

## Key Features & Contents

*   **JMLE CoT Dataset Generation Script (`scripts/jmle_gemini_cot_generator.py`)**:
    *   Utilizes the Google Gemini API (gemini-2.5-pro) to generate thought processes (Chain-of-Thought, CoT) and explanations from JMLE questions.
    *   Separates successfully generated samples from failed ones (e.g., where the generated answer doesn't match the original correct answer) into distinct outputs.
    *   Includes functionality for automatic uploading to the Hugging Face Hub.
*   **JMLE DPO Dataset Generation Script (`scripts/jmle_gemini_dpo_generator.py`)**:
    *   Generates datasets suitable for Direct Preference Optimization (DPO) training based on an existing Supervised Fine-Tuning (SFT) dataset (e.g., the CoT dataset generated and corrected above).
    *   Creates pairs of high-quality responses (chosen) and intentionally generated lower-quality responses (rejected).
    *   Features automatic uploading to the Hugging Face Hub.
*   **CoT Dataset Correction Colab Notebook (`colab_notebooks/JMLE-Gemini-2.5-Pro-CoT-Dataset-Correction.ipynb`)**:
    *   Provides an interface for manually reviewing and correcting the dataset generated by `jmle_gemini_cot_generator.py`, especially the `unmatched` data where answer mismatches occurred.
    *   Includes functionality to merge the corrected data with the original successful data and upload the combined dataset to the Hugging Face Hub.
*   **Model Training Sample Colab Notebooks**:
    *   **`colab_notebooks/JMLE-CPT.ipynb`**: A sample notebook demonstrating Continued Pre-Training (CPT) using the generated datasets.
    *   **`colab_notebooks/JMLE-SFT.ipynb`**: A sample notebook demonstrating Supervised Fine-Tuning (SFT) using the generated datasets.
*   **Script Details**: Refer to `scripts/README.md` for detailed options and usage instructions for each script.

## Directory Structure

```
├── .gitignore                 # Git ignore list
├── JMLE-LLM-Toolkit.code-workspace # VS Code workspace settings (Formerly: Med-LLM-Jp)
├── README.md                  # This file
├── colab_notebooks/           # Notebooks for dataset post-processing and training
│   ├── JMLE-CPT.ipynb         # Sample notebook for CPT (Continued Pre-Training)
│   ├── JMLE-Gemini-2.5-Pro-CoT-Dataset-Correction.ipynb # Notebook for CoT dataset post-processing/correction
│   └── JMLE-SFT.ipynb         # Sample notebook for SFT (Supervised Fine-Tuning)
├── pyproject.toml             # Python project configuration (for uv)
├── scripts/                   # Scripts for dataset generation
│   ├── README.md              # Detailed script descriptions
│   ├── jmle_gemini_cot_generator.py # CoT dataset generation script
│   └── jmle_gemini_dpo_generator.py # DPO dataset generation script
└── uv.lock                    # Dependency lock file (for uv)
```

## Setup

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/docto-rin/JMLE-LLM-Toolkit.git
    cd JMLE-LLM-Toolkit
    ```
    *(Note: Replace `docto-rin` in the URL above with your actual GitHub username or organization name)*

2.  **Set up environment variables:**
    Create a `.env` file in the project root and set the necessary API keys and tokens:
    ```env
    # Required for data generation
    GEMINI_API_KEY=YOUR_GEMINI_API_KEY

    # Required for uploading to Hugging Face Hub (write permission needed)
    HF_TOKEN=YOUR_HUGGINGFACE_WRITE_TOKEN

    # Optional: For Slack notifications (uses py2slack)
    # SLACK_OAUTH_TOKEN=YOUR_SLACK_OAUTH_TOKEN
    # SLACK_DEFAULT_CHANNEL=your-slack-channel
    ```
    *   Obtain `GEMINI_API_KEY` from Google AI Studio or your Google Cloud project.
    *   Create a `HF_TOKEN` with `write` permissions in your Hugging Face account settings (`Settings` -> `Access Tokens`).
    *   If using Slack notifications, refer to the [py2slack](https://github.com/docto-rin/py2slack) documentation to set up `SLACK_OAUTH_TOKEN` and `SLACK_DEFAULT_CHANNEL`.

3.  **Install dependencies:**
    Use [uv](https://github.com/astral-sh/uv) to install the dependencies.
    ```bash
    uv sync
    ```

## Usage

### 1. Generate CoT Dataset

Run the `scripts/jmle_gemini_cot_generator.py` script. Refer to `scripts/README.md` for detailed options.

**Example Usage (process 10 samples, use system prompt, upload to Hub):**
```bash
uv run python scripts/jmle_gemini_cot_generator.py --sample_size 10 --use_system_prompt --upload_to_hub --hub_repo [YourHubUsername]/JMLE-CoT-gemini-2.5-pro-dataset-test --private_repo
```
*(Note: Replace `[YourHubUsername]/JMLE-CoT-gemini-2.5-pro-dataset-test` with your desired repository ID)*

### 2. Generate DPO Dataset

Run the `scripts/jmle_gemini_dpo_generator.py` script. Specify the input SFT dataset (e.g., the corrected CoT dataset generated above). Refer to `scripts/README.md` for detailed options.

**Example Usage (using corrected CoT data as input, saving interim results, uploading to Hub):**
```bash
uv run python scripts/jmle_gemini_dpo_generator.py \
    --sft_dataset_id "[YourHubUsername]/JMLE-CoT-gemini-2.5-pro-dataset-combined" \
    --output_dir "JMLE-DPO-gemini-2.5-pro-dataset" \
    --save_interim \
    --interim_interval 100 \
    --upload_to_hub \
    --hub_repo_id "[YourHubUsername]/JMLE-DPO-gemini-2.5-pro-dataset" \
    --hub_private
```
*(Note: Replace `[YourHubUsername]/JMLE-CoT-gemini-2.5-pro-dataset-combined` and `[YourHubUsername]/JMLE-DPO-gemini-2.5-pro-dataset` with your appropriate repository IDs)*

### 3. Correct CoT Dataset

Open and run `colab_notebooks/JMLE-Gemini-2.5-Pro-CoT-Dataset-Correction.ipynb` in Google Colab.
Follow the instructions within the notebook to manually correct the data and upload the corrected/combined dataset to the Hugging Face Hub.

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/docto-rin/JMLE-LLM-Toolkit/blob/main/colab_notebooks/JMLE-Gemini-2.5-Pro-CoT-Dataset-Correction.ipynb)

### 4. Model Training (Samples)

The following Colab notebooks provide examples of how to use the generated datasets for model training:

*   **CPT (Continued Pre-Training):** `colab_notebooks/JMLE-CPT.ipynb`
    [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/docto-rin/JMLE-LLM-Toolkit/blob/main/colab_notebooks/JMLE-CPT.ipynb)
*   **SFT (Supervised Fine-Tuning):** `colab_notebooks/JMLE-SFT.ipynb`
    [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/docto-rin/JMLE-LLM-Toolkit/blob/main/colab_notebooks/JMLE-SFT.ipynb)

*(Note: Replace `docto-rin` in the Colab badge URLs with your GitHub username or organization name)*

## Generated Datasets (Examples)

The datasets generated and managed by the tools in this repository are (or can be made) available on the Hugging Face Hub. Repository names may vary based on script arguments or notebook settings.

*   **Generated CoT Data:** `[YourHubUsername]/JMLE-CoT-gemini-2.5-pro-dataset`
*   **Corrected & Combined CoT Data:** `[YourHubUsername]/JMLE-CoT-gemini-2.5-pro-dataset-combined`
*   **Generated DPO Data:** `[YourHubUsername]/JMLE-DPO-gemini-2.5-pro-dataset`

## License

This repository is licensed under the Apache License 2.0. For details, see the [LICENSE](LICENSE) file.

## Disclaimer / Notes

*   Dataset generation requires a Google Gemini API key and a Hugging Face Hub token.
*   Using the APIs may incur costs.
*   Users are responsible for verifying the accuracy and ethical implications of the generated content.

# -*- coding: utf-8 -*-
"""JMLE-Gemini-2.5-Pro-CoT-Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1envXvoseFv46h4ViynIUdw4MfXeuiv8D

## Config and Installation
"""

import os
from dotenv import load_dotenv
from huggingface_hub import login as hf_login, HfApi, HfFolder
from datasets import load_dataset, DatasetDict
from openai import OpenAI
import re
from py2slack import slack_notify, send_slack

# 環境変数の読み込み
load_dotenv(override=True)

# シークレットの使用設定
use_secret = True

# トークンの取得
hf_token = os.getenv('HF_TOKEN') if use_secret else None
gemini_api_key = os.getenv('GEMINI_API_KEY') if use_secret else None

# Hugging Faceへのログイン
if hf_token:
    hf_login(hf_token)
else:
    hf_login()

# トークンの確認
token = HfFolder.get_token()
api = HfApi()
user_info = api.whoami(token=token)
print(user_info)

"""## Dataset"""

# データセットの読み込み
dataset = load_dataset("longisland3/NMLE")

# 最初のサンプルの内容を表示
for k, v in dataset["train"][0].items():
    print(f"{k}: {v}")

"""## Setup"""

# OpenAIクライアントの設定
model_name = "gemini-2.5-pro-exp-03-25"
config = {
    "api_key": gemini_api_key,
    "base_url": "https://generativelanguage.googleapis.com/v1beta/",
    "model_name": model_name,
    "client_type": "openai",
    "supports_vision": True,
    "system_role": "system",
    "parameters": {}
}
client = OpenAI(api_key=config["api_key"], base_url=config["base_url"])

# プロンプトの作成関数
def create_prompt(example: dict) -> str:
    return (
        "あなたは医師国家試験の過去問を解く、医学を隅々まで理解している優秀で論理的なアシスタントです。\n"
        f"問題: {example['question']}\n"
        f"選択肢: {', '.join(example['choices'])}\n"
        f"正解: {''.join(sorted(example['answer']))}\n\n"
        "普段は<Thoughts></Thoughts>タグで囲まれた思考過程は出力しないかと思いますが、今回はその部分も含めて以下のような形式で出力してほしいです。"
        "【出力形式】\n"
        "<Thoughts>[ここに詳細な問題解決の過程をステップごとに記述。ただし出力形式の確認についての文はここに含めなくていいです。]</Thoughts>\n"
        "<Output>\n"
        "answer: [最終的な答え（例: abe）]\n"
        "explanation: [その答えを選んだ根拠となる医学的な解説を簡潔に]\n"
        "</Output>\n\n"
        "指定されたタグは必ず閉じ、余分なMarkdown記法や装飾は含めないでください。"
    )

# Gemini API呼び出し関数
def call_gemini(prompt: str, client=client, config=config) -> str:
    messages = [{"role": "user", "content": prompt}]
    try:
        response = client.chat.completions.create(
            model=config["model_name"],
            messages=messages,
            **config["parameters"]
        )
        if response and response.choices:
            return response.choices[0].message.content
        else:
            return None
    except Exception as e:
        print(f"An error occurred: {str(e)}")
        return None

"""## Demo"""

# 最初のサンプルでデモを実行
example = dataset["train"][0]
prompt = create_prompt(example)
response = call_gemini(prompt)
print(response)

"""## Main"""

# サンプルの処理関数
def process_example(example):
    original_answer = ''.join(sorted(example['answer']))
    prompt = create_prompt(example)
    max_retries = 3
    attempt = 0

    while attempt < max_retries:
        response = call_gemini(prompt)
        if response is None:
            print(f"Attempt {attempt+1}: No response received. Retrying...")
            attempt += 1
            continue

        thoughts_match = re.search(r"<Thoughts>(.*?)</Thoughts>", response, re.DOTALL)
        output_match = re.search(r"<Output>(.*?)</Output>", response, re.DOTALL)
        if output_match:
            output_content = output_match.group(1)
            answer_match = re.search(r"answer:\s*(.*)", output_content)
            explanation_match = re.search(r"explanation:\s*(.*)", output_content)
        else:
            answer_match = None
            explanation_match = None

        extracted_answer = answer_match.group(1).strip() if answer_match else ""
        if extracted_answer == original_answer:
            example["cot"] = thoughts_match.group(1).strip() if thoughts_match else ""
            example["answer_updated"] = original_answer
            example["explanation_updated"] = explanation_match.group(1).strip() if explanation_match else ""
            return example
        else:
            print(f"Attempt {attempt+1}: Extracted answer '{extracted_answer}' does not match original answer '{original_answer}'. Retrying...")
            attempt += 1

    print(f"Skipping example after {max_retries} failed attempts.")
    return None

@slack_notify
def main():
    processed_train = dataset["train"].map(process_example, remove_columns=dataset["train"].column_names)

    # Noneのサンプルをフィルタリング
    processed_train = processed_train.filter(lambda x: x is not None)

    processed_dataset = DatasetDict({
        "train": processed_train,
    })

    send_slack("NMLE-CoT-Updated", "NMLE-CoT-Updated dataset processing completed.")
    processed_dataset.push_to_hub("doctorin/JMLE-CoT-gemini-2.5-pro-exp-03-25", private=True)

main()
